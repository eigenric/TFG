{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📑 Tutorials for PyPOTS Imputation Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📀 Preparing the **PhysioNet-2012** dataset for this tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eigenric/.local/share/virtualenvs/TFG--n4J6ECg/lib/python3.11/site-packages/pypots/nn/modules/reformer/local_attention.py:31: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled=False)\n",
      "/Users/eigenric/.local/share/virtualenvs/TFG--n4J6ECg/lib/python3.11/site-packages/pypots/nn/modules/reformer/local_attention.py:98: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled=False)\n",
      "2025-02-24 17:38:11 [INFO]: Have set the random seed as 16 for numpy and pytorch.\n",
      "2025-02-24 17:38:11 [INFO]: You're using dataset physionet_2012, please cite it properly in your work. You can find its reference information at the below link: \n",
      "https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/physionet_2012\n",
      "2025-02-24 17:38:11 [INFO]: Dataset physionet_2012 has already been downloaded. Processing directly...\n",
      "2025-02-24 17:38:11 [INFO]: Dataset physionet_2012 has already been cached. Loading from cache directly...\n",
      "2025-02-24 17:38:11 [INFO]: Loaded successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\n",
      "████████╗██╗███╗   ███╗███████╗    ███████╗███████╗██████╗ ██╗███████╗███████╗    █████╗ ██╗\n",
      "╚══██╔══╝██║████╗ ████║██╔════╝    ██╔════╝██╔════╝██╔══██╗██║██╔════╝██╔════╝   ██╔══██╗██║\n",
      "   ██║   ██║██╔████╔██║█████╗█████╗███████╗█████╗  ██████╔╝██║█████╗  ███████╗   ███████║██║\n",
      "   ██║   ██║██║╚██╔╝██║██╔══╝╚════╝╚════██║██╔══╝  ██╔══██╗██║██╔══╝  ╚════██║   ██╔══██║██║\n",
      "   ██║   ██║██║ ╚═╝ ██║███████╗    ███████║███████╗██║  ██║██║███████╗███████║██╗██║  ██║██║\n",
      "   ╚═╝   ╚═╝╚═╝     ╚═╝╚══════╝    ╚══════╝╚══════╝╚═╝  ╚═╝╚═╝╚══════╝╚══════╝╚═╝╚═╝  ╚═╝╚═╝\n",
      "ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai \u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-24 17:38:24 [WARNING]: Note that physionet_2012 has sparse observations in the time series, hence we don't add additional missing values to the training dataset. \n",
      "2025-02-24 17:38:24 [INFO]: 69461 values masked out in the val set as ground truth, take 10.02% of the original observed values\n",
      "2025-02-24 17:38:25 [INFO]: 86037 values masked out in the test set as ground truth, take 10.01% of the original observed values\n",
      "2025-02-24 17:38:25 [INFO]: Total sample number: 11988\n",
      "2025-02-24 17:38:25 [INFO]: Training set size: 7671 (63.99%)\n",
      "2025-02-24 17:38:25 [INFO]: Validation set size: 1918 (16.00%)\n",
      "2025-02-24 17:38:25 [INFO]: Test set size: 2399 (20.01%)\n",
      "2025-02-24 17:38:25 [INFO]: Number of steps: 48\n",
      "2025-02-24 17:38:25 [INFO]: Number of features: 37\n",
      "2025-02-24 17:38:25 [INFO]: Train set missing rate: 79.69%\n",
      "2025-02-24 17:38:25 [INFO]: Validating set missing rate: 81.70%\n",
      "2025-02-24 17:38:25 [INFO]: Test set missing rate: 81.84%\n",
      "2025-02-24 17:38:25 [WARNING]: 🚨 BenchPOTS package now is fully released and includes preprocessing functions for 170+ datasets. gene_physionet2012() has been deprecated and will be removed in pypots v0.9\n",
      "2025-02-24 17:38:25 [INFO]: 🌟 Please refer to https://github.com/WenjieDu/BenchPOTS and check out the func benchpots.datasets.preprocess_physionet2012()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['n_classes', 'n_steps', 'n_features', 'scaler', 'train_X', 'train_y', 'train_ICUType', 'val_X', 'val_y', 'val_ICUType', 'test_X', 'test_y', 'test_ICUType', 'val_X_ori', 'test_X_ori'])\n"
     ]
    }
   ],
   "source": [
    "from pypots.data.generating import gene_physionet2012\n",
    "from pypots.utils.random import set_random_seed\n",
    "from global_config import RANDOM_SEED\n",
    "\n",
    "set_random_seed(RANDOM_SEED)\n",
    "\n",
    "# Load the PhysioNet-2012 dataset\n",
    "physionet2012_dataset = gene_physionet2012(artificially_missing_rate=0.1)\n",
    "\n",
    "# Take a look at the generated PhysioNet-2012 dataset, you'll find that everything has been prepared for you,\n",
    "# data splitting, normalization, additional artificially-missing values for evaluation, etc.\n",
    "print(physionet2012_dataset.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble the datasets for training, validating, and testing.\n",
    "\n",
    "dataset_for_training = {\n",
    "    \"X\": physionet2012_dataset['train_X'],\n",
    "}\n",
    "\n",
    "dataset_for_validating = {\n",
    "    \"X\": physionet2012_dataset['val_X'],\n",
    "    \"X_ori\": physionet2012_dataset['val_X_ori'],\n",
    "}\n",
    "\n",
    "dataset_for_testing = {\n",
    "    \"X\": physionet2012_dataset['test_X'],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[            nan,             nan,             nan, ...,\n",
       "                     nan, -3.31084528e+00,             nan],\n",
       "        [            nan,             nan,             nan, ...,\n",
       "                     nan,             nan,             nan],\n",
       "        [            nan,             nan,             nan, ...,\n",
       "                     nan,             nan,             nan],\n",
       "        ...,\n",
       "        [            nan,             nan,             nan, ...,\n",
       "                     nan,             nan,             nan],\n",
       "        [            nan,             nan,             nan, ...,\n",
       "                     nan,             nan,             nan],\n",
       "        [            nan,             nan,             nan, ...,\n",
       "                     nan,             nan,             nan]],\n",
       "\n",
       "       [[            nan,             nan,             nan, ...,\n",
       "                     nan, -2.95065256e-01,  2.15766049e-03],\n",
       "        [            nan,             nan,             nan, ...,\n",
       "         -4.73794630e-01,             nan,  5.07372268e-04],\n",
       "        [            nan,             nan,             nan, ...,\n",
       "                     nan,             nan,             nan],\n",
       "        ...,\n",
       "        [            nan,             nan,             nan, ...,\n",
       "                     nan, -7.57357996e-02, -1.10446453e-02],\n",
       "        [            nan,             nan,             nan, ...,\n",
       "                     nan, -7.57357996e-02,             nan],\n",
       "        [            nan,             nan,             nan, ...,\n",
       "                     nan, -7.57357996e-02,             nan]],\n",
       "\n",
       "       [[            nan,             nan,             nan, ...,\n",
       "                     nan, -3.31084528e+00,             nan],\n",
       "        [            nan,             nan,             nan, ...,\n",
       "                     nan,             nan,             nan],\n",
       "        [            nan,             nan,             nan, ...,\n",
       "                     nan,             nan,             nan],\n",
       "        ...,\n",
       "        [            nan,             nan,             nan, ...,\n",
       "                     nan,             nan,             nan],\n",
       "        [            nan,             nan,             nan, ...,\n",
       "                     nan,             nan,             nan],\n",
       "        [            nan,             nan,             nan, ...,\n",
       "                     nan,             nan,             nan]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[            nan,             nan,             nan, ...,\n",
       "                     nan,  4.87484336e+00,             nan],\n",
       "        [            nan,             nan,             nan, ...,\n",
       "                     nan,             nan,             nan],\n",
       "        [            nan,             nan,             nan, ...,\n",
       "                     nan,             nan,             nan],\n",
       "        ...,\n",
       "        [            nan,             nan,             nan, ...,\n",
       "                     nan,             nan,             nan],\n",
       "        [            nan,             nan,             nan, ...,\n",
       "                     nan,             nan,             nan],\n",
       "        [            nan,             nan,             nan, ...,\n",
       "                     nan,             nan,             nan]],\n",
       "\n",
       "       [[            nan,             nan,             nan, ...,\n",
       "                     nan,  5.74285131e-02, -1.14291596e-03],\n",
       "        [            nan,             nan,             nan, ...,\n",
       "         -6.36717535e-01,             nan,             nan],\n",
       "        [            nan,             nan,             nan, ...,\n",
       "                     nan,             nan,             nan],\n",
       "        ...,\n",
       "        [            nan,             nan,             nan, ...,\n",
       "                     nan,  3.70756308e-01,             nan],\n",
       "        [            nan,             nan,             nan, ...,\n",
       "                     nan,             nan,             nan],\n",
       "        [            nan,             nan,             nan, ...,\n",
       "                     nan,  3.70756308e-01,             nan]],\n",
       "\n",
       "       [[            nan,             nan,             nan, ...,\n",
       "                     nan, -4.08646581e-01, -2.91978158e-02],\n",
       "        [-3.12440071e-01, -3.11331247e-01, -2.95247315e-01, ...,\n",
       "          5.63767031e-01,             nan,             nan],\n",
       "        [            nan,             nan,             nan, ...,\n",
       "                     nan,             nan,             nan],\n",
       "        ...,\n",
       "        [            nan,             nan,             nan, ...,\n",
       "                     nan,             nan,             nan],\n",
       "        [            nan,             nan,             nan, ...,\n",
       "                     nan,             nan,             nan],\n",
       "        [            nan,             nan,             nan, ...,\n",
       "                     nan,             nan,             nan]]],\n",
       "      shape=(7671, 48, 37))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_for_training['X']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "physionet2012_dataset['n_steps']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "physionet2012_dataset['n_features']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 An example of **SAITS** for imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-24 17:38:25 [INFO]: No given device, using default device: cpu\n",
      "2025-02-24 17:38:25 [INFO]: Model files will be saved to tutorial_results/imputation/saits/20250224_T173825\n",
      "2025-02-24 17:38:25 [INFO]: Tensorboard file will be saved to tutorial_results/imputation/saits/20250224_T173825/tensorboard\n",
      "2025-02-24 17:38:25 [INFO]: SAITS initialized with the given hyperparameters, the number of trainable parameters: 1,378,358\n"
     ]
    }
   ],
   "source": [
    "from pypots.optim import Adam\n",
    "from pypots.imputation import SAITS\n",
    "\n",
    "# initialize the model\n",
    "saits = SAITS(\n",
    "    n_steps=physionet2012_dataset['n_steps'],\n",
    "    n_features=physionet2012_dataset['n_features'],\n",
    "    n_layers=2,\n",
    "    d_model=256,\n",
    "    d_ffn=128,\n",
    "    n_heads=4,\n",
    "    d_k=64,\n",
    "    d_v=64,\n",
    "    dropout=0.1,\n",
    "    attn_dropout=0.1,\n",
    "    ##### Esto diferencia al SAIT del modelo Transformer\n",
    "    diagonal_attention_mask=True,  # otherwise the original self-attention mechanism will be applied\n",
    "    ORT_weight=1,  # you can adjust the weight values of arguments ORT_weight\n",
    "    # and MIT_weight to make the SAITS model focus more on one task. Usually you can just leave them to the default values, i.e. 1.\n",
    "    MIT_weight=1,\n",
    "    batch_size=32,\n",
    "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "    epochs=10,\n",
    "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "    # You can leave it to defualt as None to disable early stopping.\n",
    "    patience=3,\n",
    "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "    optimizer=Adam(lr=1e-3),\n",
    "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "    num_workers=0,\n",
    "    # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
    "    # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
    "    device=None,  \n",
    "    # set the path for saving tensorboard and trained model files \n",
    "    saving_path=\"tutorial_results/imputation/saits\",\n",
    "    # only save the best model after training finished.\n",
    "    # You can also set it as \"better\" to save models performing better ever during training.\n",
    "    model_saving_strategy=\"best\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-24 17:39:23 [INFO]: Epoch 001 - training loss: 0.7263, validation loss: 0.3443\n",
      "2025-02-24 17:40:15 [INFO]: Epoch 002 - training loss: 0.5407, validation loss: 0.3227\n",
      "2025-02-24 17:41:22 [INFO]: Epoch 003 - training loss: 0.4928, validation loss: 0.2973\n",
      "2025-02-24 17:42:08 [INFO]: Epoch 004 - training loss: 0.4589, validation loss: 0.2726\n",
      "2025-02-24 17:42:50 [INFO]: Epoch 005 - training loss: 0.4330, validation loss: 0.2637\n",
      "2025-02-24 17:43:31 [INFO]: Epoch 006 - training loss: 0.4131, validation loss: 0.2581\n",
      "2025-02-24 17:44:16 [INFO]: Epoch 007 - training loss: 0.3959, validation loss: 0.2482\n",
      "2025-02-24 17:45:03 [INFO]: Epoch 008 - training loss: 0.3841, validation loss: 0.2512\n",
      "2025-02-24 17:46:08 [INFO]: Epoch 009 - training loss: 0.3759, validation loss: 0.2401\n",
      "2025-02-24 17:46:52 [INFO]: Epoch 010 - training loss: 0.3718, validation loss: 0.2395\n",
      "2025-02-24 17:46:52 [INFO]: Finished training. The best model is from epoch#10.\n",
      "2025-02-24 17:46:52 [INFO]: Saved the model to tutorial_results/imputation/saits/20250224_T173825/SAITS.pypots\n"
     ]
    }
   ],
   "source": [
    "# train the model on the training set, and validate it on the validating set to select the best model for testing in the next step\n",
    "saits.fit(train_set=dataset_for_training, val_set=dataset_for_validating)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the testing stage, impute the originally-missing values and artificially-missing values in the test set\n",
    "\n",
    "saits_results = saits.predict(dataset_for_testing)\n",
    "saits_imputation = saits_results[\"imputation\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-3.39879394e-01, -4.37852383e-01, -4.21492040e-01, ...,\n",
       "         -8.89166966e-02, -1.05096853e+00,  7.64137646e-03],\n",
       "        [-1.47559956e-01, -2.59139180e-01, -1.85112804e-01, ...,\n",
       "         -3.11014652e-01, -1.05096853e+00,  1.15424506e-02],\n",
       "        [-4.09875810e-01, -4.41974819e-01, -4.40624833e-01, ...,\n",
       "         -6.33399338e-02, -1.05096853e+00,  4.20941040e-03],\n",
       "        ...,\n",
       "        [-3.45102191e-01, -3.58970135e-01, -3.88035297e-01, ...,\n",
       "         -1.18300311e-01, -1.05096853e+00,  3.33231990e-03],\n",
       "        [-2.90943265e-01, -3.55989158e-01, -3.90741646e-01, ...,\n",
       "         -1.04197644e-01, -1.05096853e+00,  5.07524936e-04],\n",
       "        [-3.78139615e-01, -4.38871443e-01, -4.37648267e-01, ...,\n",
       "         -1.44222736e-01, -1.05096853e+00,  1.11249043e-04]],\n",
       "\n",
       "       [[-8.17321241e-02, -3.19774091e-01, -3.06260765e-01, ...,\n",
       "         -1.22224152e-01,  4.17621247e-02,  3.61189805e-03],\n",
       "        [-4.16784525e-01, -5.25243700e-01, -4.85108107e-01, ...,\n",
       "         -8.97674412e-02,  4.17621247e-02, -8.42673518e-03],\n",
       "        [-3.13966483e-01, -4.81845558e-01, -4.03704435e-01, ...,\n",
       "         -1.42045822e-02,  4.17621247e-02, -6.44065160e-03],\n",
       "        ...,\n",
       "        [-2.58512497e-01, -4.41848099e-01, -4.51403081e-01, ...,\n",
       "         -2.11007267e-01,  4.17621247e-02,  1.60621945e-03],\n",
       "        [-2.98817128e-01, -4.39714611e-01, -4.63381618e-01, ...,\n",
       "         -2.47753501e-01,  4.17621247e-02, -2.86286883e-03],\n",
       "        [-2.29911745e-01, -4.09610510e-01, -4.21940565e-01, ...,\n",
       "         -2.14525253e-01,  4.17621247e-02, -1.48894498e-03]],\n",
       "\n",
       "       [[-3.73215675e-01, -4.67310935e-01, -4.86898839e-01, ...,\n",
       "         -2.31975820e-02,  3.78455259e-02, -1.15565881e-02],\n",
       "        [-3.19460988e-01, -4.81096327e-01, -4.51535642e-01, ...,\n",
       "          6.80740550e-03,  3.78455259e-02, -1.06242485e-02],\n",
       "        [-3.00978363e-01, -4.83570576e-01, -4.29062247e-01, ...,\n",
       "          1.57212168e-02,  3.78455259e-02, -9.37505905e-03],\n",
       "        ...,\n",
       "        [-4.26110148e-01, -5.02268076e-01, -5.72356939e-01, ...,\n",
       "         -1.14632159e-01,  3.78455259e-02, -7.14273192e-03],\n",
       "        [-2.93412566e-01, -4.77372676e-01, -4.76284862e-01, ...,\n",
       "         -1.02881670e-01,  3.78455259e-02, -2.70720012e-03],\n",
       "        [-2.94457316e-01, -4.30076599e-01, -5.19321263e-01, ...,\n",
       "         -1.07621863e-01,  3.78455259e-02, -1.02706403e-02]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-4.78525877e-01, -6.62593722e-01, -6.07396245e-01, ...,\n",
       "         -6.34517372e-02, -2.75482267e-01, -9.39435698e-03],\n",
       "        [-3.68163556e-01, -5.17215610e-01, -5.69625199e-01, ...,\n",
       "         -4.78859544e-02, -2.05537915e-01, -6.09378051e-03],\n",
       "        [-4.19822276e-01, -5.98811507e-01, -5.41508198e-01, ...,\n",
       "         -3.01895980e-02, -1.79448873e-01, -7.74406875e-03],\n",
       "        ...,\n",
       "        [-2.44015485e-01, -4.12064850e-01, -3.55312228e-01, ...,\n",
       "          1.56019228e-02,  5.35119139e-02, -7.04222545e-03],\n",
       "        [-2.85932243e-01, -4.62894142e-01, -4.15443838e-01, ...,\n",
       "         -2.84603387e-02,  5.35119139e-02, -9.82905366e-03],\n",
       "        [-2.25999460e-01, -5.19552052e-01, -3.92428517e-01, ...,\n",
       "          9.10494663e-03,  5.35119139e-02, -1.04536973e-02]],\n",
       "\n",
       "       [[-4.39372838e-01, -5.19218028e-01, -4.93432999e-01, ...,\n",
       "         -3.01100284e-01, -6.12309635e-01, -4.44883341e-03],\n",
       "        [-3.02739859e-01, -4.42372739e-01, -3.94539654e-01, ...,\n",
       "         -1.98592037e-01, -6.12309635e-01, -2.49557616e-03],\n",
       "        [-3.11118305e-01, -3.93479973e-01, -3.74645770e-01, ...,\n",
       "         -9.83357728e-02, -6.12309635e-01,  8.16360989e-04],\n",
       "        ...,\n",
       "        [-3.42495829e-01, -2.47622192e-01, -3.62811804e-01, ...,\n",
       "         -4.11781549e-01, -6.22200727e-01,  1.44448376e-03],\n",
       "        [-2.64842540e-01, -3.02701652e-01, -3.82653117e-01, ...,\n",
       "         -3.34453762e-01, -6.00559831e-01, -1.54507649e-03],\n",
       "        [-2.36861378e-01, -3.15605432e-01, -3.57722044e-01, ...,\n",
       "         -5.89584470e-01, -6.00559831e-01, -2.65290379e-03]],\n",
       "\n",
       "       [[-6.69573694e-02, -1.96205765e-01, -2.39720866e-02, ...,\n",
       "          3.73495430e-01,  1.74156547e+00, -4.19232342e-03],\n",
       "        [-3.51720512e-01, -5.04688084e-01, -3.76652747e-01, ...,\n",
       "          3.33692059e-02,  1.74156547e+00, -1.93348974e-02],\n",
       "        [-3.60048741e-01, -5.26414514e-01, -4.03871715e-01, ...,\n",
       "          6.60056621e-03,  1.74156547e+00, -1.43638356e-02],\n",
       "        ...,\n",
       "        [-2.15619072e-01, -3.33744437e-01, -3.33688289e-01, ...,\n",
       "         -9.97524112e-02,  1.89425421e+00, -1.06248586e-02],\n",
       "        [-2.99331933e-01, -4.51352537e-01, -3.92007172e-01, ...,\n",
       "         -8.90413150e-02,  1.76506495e+00, -1.28242262e-02],\n",
       "        [-2.35657617e-01, -3.87149394e-01, -3.83573443e-01, ...,\n",
       "         -1.06742457e-01,  1.76506495e+00, -1.09032355e-02]]],\n",
       "      shape=(2399, 48, 37), dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saits_imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.1621629 , -0.29536228, -0.27797275, ...,  0.01900954,\n",
       "        -0.36396021, -0.00995649],\n",
       "       [-0.16312107, -0.29983069, -0.28483136, ...,  0.01345677,\n",
       "        -0.25902407, -0.00474422],\n",
       "       [-0.16725748, -0.30267454, -0.28715961, ...,  0.01724678,\n",
       "        -0.25595969, -0.00416867],\n",
       "       ...,\n",
       "       [-0.17454517, -0.30238643, -0.28782827, ...,  0.00449406,\n",
       "        -0.12005425, -0.00915084],\n",
       "       [-0.17096113, -0.30405467, -0.28838514, ...,  0.00160955,\n",
       "        -0.12269905, -0.00932488],\n",
       "       [-0.17276722, -0.29471161, -0.27505285, ...,  0.00812919,\n",
       "        -0.13113304, -0.00932557]], shape=(48, 37))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(saits_imputation, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2399, 48, 37)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saits_imputation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de NaNs en test_X_ori después de imputación (con 0): 0\n",
      "Testing mean absolute error (MAE): 0.3018\n"
     ]
    }
   ],
   "source": [
    "from pypots.utils.metrics import calc_mae\n",
    "\n",
    "physionet2012_dataset['test_X_indicating_mask'] = np.isnan(physionet2012_dataset['test_X']).astype(int)\n",
    "\n",
    "# Crear una copia de los datos originales\n",
    "test_X_ori_fixed = np.copy(physionet2012_dataset['test_X_ori'])\n",
    "\n",
    "# Reemplazar NaNs por 0 (valor constante)\n",
    "test_X_ori_fixed[np.isnan(test_X_ori_fixed)] = 0\n",
    "\n",
    "# Verificar si quedan NaNs\n",
    "num_nans_fixed = np.isnan(test_X_ori_fixed).sum()\n",
    "print(f\"Número de NaNs en test_X_ori después de imputación (con 0): {num_nans_fixed}\")\n",
    "\n",
    "# calculate mean absolute error on the ground truth (artificially-missing values)\n",
    "# predictions, targets, masks\n",
    "testing_mae = calc_mae(\n",
    "    saits_imputation, \n",
    "    test_X_ori_fixed, \n",
    "    physionet2012_dataset['test_X_indicating_mask'],\n",
    ")\n",
    "print(f\"Testing mean absolute error (MAE): {testing_mae:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
