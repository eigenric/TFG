@article{vaswani_attention_2017,
  title = {Attention Is All You Need},
  url = {https://arxiv.org/abs/1706.03762},
  doi = {10.48550/ARXIV.1706.03762},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2017},
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG)},
}
