
@article{vaswani_attention_2017,
	title = {Attention Is All You Need},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1706.03762},
	doi = {10.48550/ARXIV.1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. },
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	urldate = {2024-05-11},
	date = {2017},
	note = {Publisher: [object Object]
Version Number: 7},
	keywords = {Computation and Language (cs.{CL}), {FOS}: Computer and information sciences, Machine Learning (cs.{LG})},
}
