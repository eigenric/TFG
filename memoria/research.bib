
@article{vaswani_attention_2017,
	title = {Attention Is All You Need},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1706.03762},
	doi = {10.48550/ARXIV.1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 {BLEU} on the {WMT} 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 {BLEU}. On the {WMT} 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art {BLEU} score of 41.8 after training for 3.5 days on eight {GPUs}, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	urldate = {2024-05-11},
	date = {2017},
	note = {Publisher: [object Object]
Version Number: 7},
	keywords = {Computation and Language (cs.{CL}), {FOS}: Computer and information sciences, Machine Learning (cs.{LG})},
}

@book{Mostafa2012,
  added-at = {2019-10-11T10:10:38.000+0200},
  author = {Abu-Mostafa, Yaser S. and Magdon-Ismail, Malik and Lin, Hsuan-Tien},
  biburl = {https://www.bibsonomy.org/bibtex/2079c807902a5b01cf801a8c7cec519ed/lopusz_kdd},
  description = {Learning From Data: Yaser S. Abu-Mostafa, Malik Magdon-Ismail, Hsuan-Tien Lin: 9781600490064: Amazon.com: Books},
  interhash = {5665353d0134ff1dea4ae32e99335a76},
  intrahash = {079c807902a5b01cf801a8c7cec519ed},
  keywords = {general_machine_learning},
  publisher = {AMLBook},
  timestamp = {2019-10-12T23:47:07.000+0200},
  title = {Learning From Data},
  year = 2012
}


@book{shalev-shwartz_understanding_2014,
	edition = {1},
	title = {Understanding Machine Learning: From Theory to Algorithms},
	rights = {https://www.cambridge.org/core/terms},
	isbn = {978-1-107-05713-5 978-1-107-29801-9},
	url = {https://www.cambridge.org/core/product/identifier/9781107298019/type/book},
	shorttitle = {Understanding Machine Learning},
	abstract = {Machine learning is one of the fastest growing areas of computer science, with far-reaching applications. The aim of this textbook is to introduce machine learning, and the algorithmic paradigms it offers, in a principled way. The book provides a theoretical account of the fundamentals underlying machine learning and the mathematical derivations that transform these principles into practical algorithms. Following a presentation of the basics, the book covers a wide array of central topics unaddressed by previous textbooks. These include a discussion of the computational complexity of learning and the concepts of convexity and stability; important algorithmic paradigms including stochastic gradient descent, neural networks, and structured output learning; and emerging theoretical concepts such as the {PAC}-Bayes approach and compression-based bounds. Designed for advanced undergraduates or beginning graduates, the text makes the fundamentals and algorithms of machine learning accessible to students and non-expert readers in statistics, computer science, mathematics and engineering.},
	publisher = {Cambridge University Press},
	author = {Shalev-Shwartz, Shai and Ben-David, Shai},
	urldate = {2024-09-22},
	date = {2014-05-19},
	langid = {english},
	doi = {10.1017/CBO9781107298019},
}

@book{shalev-shwartz_understanding_2014-1,
	edition = {1},
	title = {Understanding Machine Learning: From Theory to Algorithms},
	rights = {https://www.cambridge.org/core/terms},
	isbn = {978-1-107-05713-5 978-1-107-29801-9},
	url = {https://www.cambridge.org/core/product/identifier/9781107298019/type/book},
	shorttitle = {Understanding Machine Learning},
	abstract = {Machine learning is one of the fastest growing areas of computer science, with far-reaching applications. The aim of this textbook is to introduce machine learning, and the algorithmic paradigms it offers, in a principled way. The book provides a theoretical account of the fundamentals underlying machine learning and the mathematical derivations that transform these principles into practical algorithms. Following a presentation of the basics, the book covers a wide array of central topics unaddressed by previous textbooks. These include a discussion of the computational complexity of learning and the concepts of convexity and stability; important algorithmic paradigms including stochastic gradient descent, neural networks, and structured output learning; and emerging theoretical concepts such as the {PAC}-Bayes approach and compression-based bounds. Designed for advanced undergraduates or beginning graduates, the text makes the fundamentals and algorithms of machine learning accessible to students and non-expert readers in statistics, computer science, mathematics and engineering.},
	publisher = {Cambridge University Press},
	author = {Shalev-Shwartz, Shai and Ben-David, Shai},
	urldate = {2024-09-22},
	date = {2014-05-19},
	langid = {english},
	doi = {10.1017/CBO9781107298019},
}

@book{klenke_probability_2020,
	location = {Cham},
	title = {Probability Theory: A Comprehensive Course},
	rights = {https://www.springer.com/tdm},
	isbn = {978-3-030-56401-8 978-3-030-56402-5},
	url = {https://link.springer.com/10.1007/978-3-030-56402-5},
	series = {Universitext},
	shorttitle = {Probability Theory},
	publisher = {Springer International Publishing},
	author = {Klenke, Achim},
	urldate = {2024-09-22},
	date = {2020},
	langid = {english},
	doi = {10.1007/978-3-030-56402-5},
}

@book{rohatgi_introduction_nodate,
	title = {An Introduction to Probability and Statistics},
	author = {Rohatgi, V K and Saleh, A K Md Ehsanes},
	langid = {english},
}

@book{perez_apuntes_nodate,
	title = {Apuntes de cálculo diferencial e integral de funciones de varias variables},
	author = {Pérez, Francisco Javier},
	langid = {spanish},
}

@misc{turner_introduction_2024,
	title = {An Introduction to Transformers},
	url = {http://arxiv.org/abs/2304.10557},
	abstract = {The transformer is a neural network component that can be used to learn useful representations of sequences or sets of data-points [Vaswani et al., 2017]. The transformer has driven recent advances in natural language processing [Devlin et al., 2019], computer vision [Dosovitskiy et al., 2021], and spatio-temporal modelling [Bi et al., 2022]. There are many introductions to transformers, but most do not contain precise mathematical descriptions of the architecture and the intuitions behind the design choices are often also missing.1 Moreover, as research takes a winding path, the explanations for the components of the transformer can be idiosyncratic. In this note we aim for a mathematically precise, intuitive, and clean description of the transformer architecture. We will not discuss training as this is rather standard. We assume that the reader is familiar with fundamental topics in machine learning including multi-layer perceptrons, linear transformations, softmax functions and basic probability.},
	number = {{arXiv}:2304.10557},
	publisher = {{arXiv}},
	author = {Turner, Richard E.},
	urldate = {2024-09-22},
	date = {2024-02-08},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2304.10557 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}


@article{de_deep_2022,
	title = {Deep Generative Models in the Industrial Internet of Things: A Survey},
	volume = {18},
	rights = {https://creativecommons.org/licenses/by/4.0/legalcode},
	issn = {1551-3203, 1941-0050},
	url = {https://ieeexplore.ieee.org/document/9726814/},
	doi = {10.1109/TII.2022.3155656},
	shorttitle = {Deep Generative Models in the Industrial Internet of Things},
	abstract = {Advances in communication technologies and artiﬁcial intelligence are accelerating the paradigm of industrial Internet of Things ({IIoT}). With {IIoT} enabling continuous integration of sensors and controllers with the network, intelligent analysis of the generated Big Data is a critical requirement. Although {IIoT} is considered a subset of {IoT}, it has its own peculiarities in terms of higher levels of safety, security, and low-latency communication in an environment of critical real-time operations. Under these circumstances, discriminative deep learning ({DL}) algorithms are unsuitable due to their need for large amounts of labeled and balanced training data, uncertainty of inputs, etc. To overcome these issues, researchers have started using deep generative models ({DGMs}), which combine the ﬂexibility of {DL} with the inference power of probabilistic modeling. In this article, we review the state of the art of {DGMs} and their applicability to {IIoT}, classifying the reviewed works into the {IIoT} application areas of anomaly detection, trust-boundary protection, network trafﬁc prediction, and platform monitoring. Following an analysis of existing {IIoT} {DGM} implementations, we identify challenges (i.e., weak discriminative capability, insufﬁcient interpretability, lack of generalization ability, generated data vulnerability, privacy concern, and data complexity) that need to be investigated in order to accelerate the adoption of {DGMs} in {IIoT} and also propose some potential research directions.},
	pages = {5728--5737},
	number = {9},
	journaltitle = {{IEEE} Transactions on Industrial Informatics},
	shortjournal = {{IEEE} Trans. Ind. Inf.},
	author = {De Suparna and Bermudez-Edo, Maria and Xu, Honghui and Cai, Zhipeng},
	urldate = {2024-09-28},
	date = {2022-09},
	langid = {english},
}

@misc{smartpoqueira,
    title = {SmartPoqueira},
    url = {https://wpd.ugr.es/~smartpoqueira/},
    note = {Proyecto pionero en Europa en experimentación con tecnología inteligente para gestión sostenible de visitas a zonas rurales y parques nacionales},
    urldate = {2024-10-01},
}