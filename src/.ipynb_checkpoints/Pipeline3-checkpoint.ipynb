{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8325eead-3b0c-4ca6-a2ed-dbad1502e136",
   "metadata": {},
   "source": [
    "# Región Pampaneira\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24d9133-8de4-406e-93df-7e3461ae1dc1",
   "metadata": {},
   "source": [
    "\n",
    "Debido al largo contenido del primer Pipeline, que incluye un analisis exploratorio de los\n",
    "datos y teniendo en cuenta que hay que restringirse a la región de Pampaneira para realizar el \n",
    "el objetivo principal de generar datos mediante modelos transformer, se ha creado un Notebook\n",
    "Pipeline2.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa43d9a-56de-493c-9f92-bb6ae9f3cf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532cb3bf-22bd-4061-bc95-89b139e683dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trafico_feb22_ago23 = pd.read_csv('trafico_feb22_ago23.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5fec5f-9e79-40b3-aff2-5a42573fe2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PAM_1 y BUB\n",
    "columns = ['vehicles_PAM_1_OUT',\n",
    " 'vehicles_PAM_1_OUT_Zona_Granada',\n",
    " 'vehicles_PAM_1_OUT_Zona_Catalunia_y_Otras',\n",
    " 'vehicles_PAM_1_OUT_Zona_Andalucia_no_GR',\n",
    " 'vehicles_PAM_1_OUT_Extranjero',\n",
    " 'vehicles_PAM_1_OUT_Zona_Comunidad_de_Madrid',\n",
    " 'vehicles_PAM_1_OUT_Zona_Extremadura_y_Otras',\n",
    " 'vehicles_PAM_1_OUT_Zona_Otras',\n",
    " 'vehicles_PAM_1_OUT_5_seats',\n",
    " 'vehicles_PAM_1_OUT_+5_seats',\n",
    " 'vehicles_PAM_1_OUT_-5_seats',\n",
    " 'vehicles_PAM_1_OUT_nan_seats',\n",
    " 'vehicles_PAM_1_OUT_101-200_CO2',\n",
    " 'vehicles_PAM_1_OUT_0-100_CO2',\n",
    " 'vehicles_PAM_1_OUT_nan_CO2',\n",
    " 'vehicles_PAM_1_OUT_201-300_CO2',\n",
    " 'vehicles_PAM_1_OUT_+300_CO2',\n",
    " 'vehicles_PAM_1_IN',\n",
    " 'vehicles_PAM_1_IN_Zona_Granada',\n",
    " 'vehicles_PAM_1_IN_Zona_Catalunia_y_Otras',\n",
    " 'vehicles_PAM_1_IN_Zona_Andalucia_no_GR',\n",
    " 'vehicles_PAM_1_IN_Extranjero',\n",
    " 'vehicles_PAM_1_IN_Zona_Comunidad_de_Madrid',\n",
    " 'vehicles_PAM_1_IN_Zona_Extremadura_y_Otras',\n",
    " 'vehicles_PAM_1_IN_Zona_Otras',\n",
    " 'vehicles_PAM_1_IN_5_seats',\n",
    " 'vehicles_PAM_1_IN_+5_seats',\n",
    " 'vehicles_PAM_1_IN_-5_seats',\n",
    " 'vehicles_PAM_1_IN_nan_seats',\n",
    " 'vehicles_PAM_1_IN_101-200_CO2',\n",
    " 'vehicles_PAM_1_IN_0-100_CO2',\n",
    " 'vehicles_PAM_1_IN_nan_CO2',\n",
    " 'vehicles_PAM_1_IN_201-300_CO2',\n",
    " 'vehicles_PAM_1_IN_+300_CO2',\n",
    " 'vehicles_PAM_2_OUT',\n",
    " 'vehicles_PAM_2_OUT_Zona_Granada',\n",
    " 'vehicles_PAM_2_OUT_Zona_Catalunia_y_Otras',\n",
    " 'vehicles_PAM_2_OUT_Zona_Andalucia_no_GR',\n",
    " 'vehicles_PAM_2_OUT_Extranjero',\n",
    " 'vehicles_PAM_2_OUT_Zona_Comunidad_de_Madrid',\n",
    " 'vehicles_PAM_2_OUT_Zona_Extremadura_y_Otras',\n",
    " 'vehicles_PAM_2_OUT_Zona_Otras',\n",
    " 'vehicles_PAM_2_OUT_5_seats',\n",
    " 'vehicles_PAM_2_OUT_+5_seats',\n",
    " 'vehicles_PAM_2_OUT_-5_seats',\n",
    " 'vehicles_PAM_2_OUT_nan_seats',\n",
    " 'vehicles_PAM_2_OUT_101-200_CO2',\n",
    " 'vehicles_PAM_2_OUT_0-100_CO2',\n",
    " 'vehicles_PAM_2_OUT_nan_CO2',\n",
    " 'vehicles_PAM_2_OUT_201-300_CO2',\n",
    " 'vehicles_PAM_2_OUT_+300_CO2',\n",
    " 'vehicles_PAM_2_IN',\n",
    " 'vehicles_PAM_2_IN_Zona_Granada',\n",
    " 'vehicles_PAM_2_IN_Zona_Catalunia_y_Otras',\n",
    " 'vehicles_PAM_2_IN_Zona_Andalucia_no_GR',\n",
    " 'vehicles_PAM_2_IN_Extranjero',\n",
    " 'vehicles_PAM_2_IN_Zona_Comunidad_de_Madrid',\n",
    " 'vehicles_PAM_2_IN_Zona_Extremadura_y_Otras',\n",
    " 'vehicles_PAM_2_IN_Zona_Otras',\n",
    " 'vehicles_PAM_2_IN_5_seats',\n",
    " 'vehicles_PAM_2_IN_+5_seats',\n",
    " 'vehicles_PAM_2_IN_-5_seats',\n",
    " 'vehicles_PAM_2_IN_nan_seats',\n",
    " 'vehicles_PAM_2_IN_101-200_CO2',\n",
    " 'vehicles_PAM_2_IN_0-100_CO2',\n",
    " 'vehicles_PAM_2_IN_nan_CO2',\n",
    " 'vehicles_PAM_2_IN_201-300_CO2',\n",
    " 'vehicles_PAM_2_IN_+300_CO2',\n",
    " 'vehicles_BUB_OUT',\n",
    " 'vehicles_BUB_OUT_Zona_Granada',\n",
    " 'vehicles_BUB_OUT_Zona_Catalunia_y_Otras',\n",
    " 'vehicles_BUB_OUT_Zona_Andalucia_no_GR',\n",
    " 'vehicles_BUB_OUT_Extranjero',\n",
    " 'vehicles_BUB_OUT_Zona_Comunidad_de_Madrid',\n",
    " 'vehicles_BUB_OUT_Zona_Extremadura_y_Otras',\n",
    " 'vehicles_BUB_OUT_Zona_Otras',\n",
    " 'vehicles_BUB_OUT_5_seats',\n",
    " 'vehicles_BUB_OUT_+5_seats',\n",
    " 'vehicles_BUB_OUT_-5_seats',\n",
    " 'vehicles_BUB_OUT_nan_seats',\n",
    " 'vehicles_BUB_OUT_101-200_CO2',\n",
    " 'vehicles_BUB_OUT_0-100_CO2',\n",
    " 'vehicles_BUB_OUT_nan_CO2',\n",
    " 'vehicles_BUB_OUT_201-300_CO2',\n",
    " 'vehicles_BUB_OUT_+300_CO2',\n",
    " 'vehicles_BUB_IN',\n",
    " 'vehicles_BUB_IN_Zona_Granada',\n",
    " 'vehicles_BUB_IN_Zona_Catalunia_y_Otras',\n",
    " 'vehicles_BUB_IN_Zona_Andalucia_no_GR',\n",
    " 'vehicles_BUB_IN_Extranjero',\n",
    " 'vehicles_BUB_IN_Zona_Comunidad_de_Madrid',\n",
    " 'vehicles_BUB_IN_Zona_Extremadura_y_Otras',\n",
    " 'vehicles_BUB_IN_Zona_Otras',\n",
    " 'vehicles_BUB_IN_5_seats',\n",
    " 'vehicles_BUB_IN_+5_seats',\n",
    " 'vehicles_BUB_IN_-5_seats',\n",
    " 'vehicles_BUB_IN_nan_seats',\n",
    " 'vehicles_BUB_IN_101-200_CO2',\n",
    " 'vehicles_BUB_IN_0-100_CO2',\n",
    " 'vehicles_BUB_IN_nan_CO2',\n",
    " 'vehicles_BUB_IN_201-300_CO2',\n",
    " 'vehicles_BUB_IN_+300_CO2']\n",
    "\n",
    "trafico_feb22_ago23[\"date\"] = pd.to_datetime(trafico_feb22_ago23[\"date\"])\n",
    "trafico_feb22_ago23[\"date\"] = trafico_feb22_ago23[\"date\"].dt.tz_convert('UTC')\n",
    "\n",
    "df_orig = trafico_feb22_ago23[[\"date\"] + columns]\n",
    "columnas_int64 = df_orig.select_dtypes(include='int64').columns\n",
    "\n",
    "# Convertir esas columnas a float64\n",
    "df_orig[columnas_int64] = df_orig[columnas_int64].astype('float64')\n",
    "\n",
    "df_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27298f48-b01d-419c-86f2-0a1d0e9f84ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "trafico_contamina_interseccion = pd.read_csv('trafico_contamina_intersección.csv')\n",
    "trafico_contamina_interseccion[\"Date\"] = pd.to_datetime(trafico_contamina_interseccion[\"Date\"])\n",
    "trafico_contamina_interseccion[\"Date\"] = trafico_contamina_interseccion[\"Date\"].dt.tz_localize(\"UTC\")\n",
    "trafico_contamina_interseccion.rename(columns={'Date': 'date'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed1ce91-7340-44cd-95a7-6e9eedba8d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_interseccion = [\"CO\", \"NO2\", \"NO\", \"O3\", \"SO2\", \"PM10\", \"eBC_tot\", \"eBC_ff\", \"eBC_bb\", \"TEMP\", \"RH\", \"WS\", \"WD\", \"PRES\", \"eBC\"]\n",
    "\n",
    "trafico_pam = trafico_contamina_interseccion[trafico_contamina_interseccion[\"truck_pos\"] == \"PAM_2\"]\n",
    "\n",
    "df_int = trafico_pam[[\"date\"] + columns_interseccion]\n",
    "columnas_int64 = df_int.select_dtypes(include='int64').columns\n",
    "\n",
    "# Convertir esas columnas a float64\n",
    "df_int[columnas_int64] = df_int[columnas_int64].astype('float64')\n",
    "df_int"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eca9977-251a-4a66-86c3-666610d0efba",
   "metadata": {},
   "source": [
    "## Interpolación para completar los valores nulos.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b262902-9b68-40db-9893-bc90d44ca5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Pintar cada una de las series temporales\n",
    "# Eje X: el tiempo y el valor Y de la serie. para ver datos pérdidos\n",
    "# ¿Hay Ráfagas? Secuencias de datos NaNs\n",
    "\n",
    "# Hacer plot de los periodos por ser\n",
    "# - 17/01/2023-14/03/2023\n",
    "# - 06/06/2023-27/06/2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29979941-0a18-4eef-b270-536aeea1728c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "start_date = pd.to_datetime(\"2023-01-17 00:00:00+00:00\")\n",
    "end_date = pd.to_datetime(\"2023-01-20 00:00:00+00:00\")\n",
    "\n",
    "subset = df_int[(df_int['date'] >= start_date) & (df_int['date'] <= end_date)]\n",
    "\n",
    "subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18ed987-b43b-42c1-9fc7-7b952cfd58da",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = pd.to_datetime(\"2023-01-16 00:00:00+00:00\")\n",
    "end_date = pd.to_datetime(\"2023-01-20 00:00:00+00:00\")\n",
    "\n",
    "subset = df_int[(df_int['date'] >= start_date) & (df_int['date'] <= end_date)]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(subset['date'], subset['CO'], marker='o', linestyle='-', label='Contaminación')\n",
    "plt.title('Serie Temporal: Contaminación')\n",
    "plt.xlabel('Fecha')\n",
    "plt.ylabel('Nivel de Contaminación')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35491df8-bc07-4a62-a411-91dc9a5a1aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar los datos\n",
    "# En los datos del camión, poner un registro por cada hora. \n",
    "# Si no se encuentra, añadir todos NaN.\n",
    "\n",
    "# Plotear una a una las variables\n",
    "# Se puede ver en la tabla directamente "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cd6234-e507-4278-a621-dd7e050a539b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Supongamos que el DataFrame inicial es df_int con columnas 'date' y 'CO'\n",
    "\n",
    "# Definir los dos períodos\n",
    "start_date_1 = pd.to_datetime(\"2023-01-17 17:00:00+00:00\")\n",
    "end_date_1 = pd.to_datetime(\"2023-03-13 11:00:00+00:00\")\n",
    "\n",
    "start_date_2 = pd.to_datetime(\"2023-06-06 13:00:00+00:00\")\n",
    "end_date_2 = pd.to_datetime(\"2023-06-27 00:00:00+00:00\")\n",
    "\n",
    "# Crear un rango completo de fechas con frecuencia horaria para ambos períodos combinados\n",
    "full_date_range = pd.date_range(start=start_date_1, end=end_date_1, freq='h').union(\n",
    "    pd.date_range(start=start_date_2, end=end_date_2, freq='h')\n",
    ")\n",
    "\n",
    "# Asegurar que 'date' sea de tipo datetime\n",
    "df_int['date'] = pd.to_datetime(df_int['date'])\n",
    "\n",
    "# Reindexar para completar las horas faltantes\n",
    "df_int = df_int.set_index('date').reindex(full_date_range).reset_index()\n",
    "df_int = df_int.rename(columns={'index': 'date'})\n",
    "\n",
    "\n",
    "df_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a044ac5-d9fd-4296-88b9-d6a44576d99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar los datos para cada período\n",
    "period_1 = df_int[(df_int['date'] >= start_date_1) & (df_int['date'] <= end_date_1)]\n",
    "period_2 = df_int[(df_int['date'] >= start_date_2) & (df_int['date'] <= end_date_2)]\n",
    "\n",
    "# Graficar ambas series temporales\n",
    "plt.figure(figsize=(12, 6))\n",
    "# Filtrar los datos para cada período\n",
    "period_1 = df_int[(df_int['date'] >= start_date_1) & (df_int['date'] <= end_date_1)]\n",
    "period_2 = df_int[(df_int['date'] >= start_date_2) & (df_int['date'] <= end_date_2)]\n",
    "\n",
    "# Graficar cada período en una imagen separada\n",
    "# Gráfica para el período 1\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(period_1['date'], period_1['CO'], marker='o', linestyle='-', color='blue', label='Período 1 (17/01 - 14/03)')\n",
    "plt.title('Serie Temporal: CO (Período 1)')\n",
    "plt.xlabel('Fecha')\n",
    "plt.ylabel('Nivel de Contaminación')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Gráfica para el período 2\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(period_2['date'], period_2['CO'], marker='o', linestyle='-', color='orange', label='Período 2 (06/06 - 27/06)')\n",
    "plt.title('Serie Temporal: CO (Período 2)')\n",
    "plt.xlabel('Fecha')\n",
    "plt.ylabel('Nivel de Contaminación (CO)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304ecaa2-b802-432c-954e-d3a0db60b538",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Supongamos que el DataFrame inicial es df_int con columnas 'date' y 'CO'\n",
    "start_date = pd.to_datetime(\"2023-01-16 17:00:00+00:00\")\n",
    "end_date = pd.to_datetime(\"2023-01-20 00:00:00+00:00\")\n",
    "\n",
    "# Crear un rango completo de fechas con frecuencia horaria\n",
    "full_date_range = pd.date_range(start=start_date, end=end_date, freq='H')\n",
    "\n",
    "# Reindexar para incluir todas las fechas del rango y rellenar con NaN los valores faltantes\n",
    "df_int = df_int.set_index('date').reindex(full_date_range).reset_index()\n",
    "\n",
    "# Renombrar las columnas\n",
    "df_int = df_int.rename(columns={'index': 'date'})\n",
    "\n",
    "df_int"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44aa88a-4d38-44bb-930e-f85e18e512e7",
   "metadata": {},
   "source": [
    "Notamos que en el conjunto interseccion todos los valores de `eBC_tot` son nulos. \n",
    "\n",
    "Por tanto eliminamos la columna,interpolamos `eBC_ff` y `eBC_bb`y luego fijamos `eBC_tot` a la suma de ambos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9c15c9-f300-4415-9cdb-01c3fadf7ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cols = [\"CO\", \"NO2\", \"NO\", \"O3\", \"SO2\", \"PM10\", \"eBC_ff\", \"eBC_bb\", \"TEMP\", \"RH\", \"WS\", \"WD\", \"PRES\", \"eBC\"]\n",
    "\n",
    "# Verificar cuántos valores no NaN hay en cada columna\n",
    "non_nan_counts = df_int[cols].notna().sum()\n",
    "print(\"Valores no NaN por columna:\")\n",
    "print(non_nan_counts)\n",
    "\n",
    "for col in cols:\n",
    "    if df_int[col].notna().sum() > 1: \n",
    "        df_int[col] = df_int[col].interpolate(method=\"spline\", order=3, limit_direction=\"both\")\n",
    "        if col in ('eBC_bb', 'SO2', \"PM10\"):\n",
    "            df_int[col] = df_int[col].clip(lower=0)  # Asegura que los valores sean >= 0\n",
    "        \n",
    "    # Rellenar valores NaN al principio o al final con el valor más cercano\n",
    "    #df_int[col] = df_int[col].fillna(method='bfill').fillna(method='pad')\n",
    "\n",
    "# Verificar el número de valores NaN después de los intentos de interpolación\n",
    "nan_count_after_interpolation = df_int[cols].isna().sum()\n",
    "print(\"\\nValores NaN después de la interpolación y relleno:\")\n",
    "print(nan_count_after_interpolation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fef3a39-0096-4385-aebd-732c672276ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_int[\"eBC_tot\"] = df_int[\"eBC_ff\"] + df_int[\"eBC_bb\"]\n",
    "df_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d18d72-1fcf-455b-99b7-3e08a480e0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Aumentar los datos \n",
    "## \n",
    "## Validar los datos con el segundo periodo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e32f07-37fc-4b6e-ac98-0005f2a636b0",
   "metadata": {},
   "source": [
    "## Normalización"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3288e36-26f9-459c-961a-907538a7e50d",
   "metadata": {},
   "source": [
    "## RobustScaler vs MinMaxScaler\n",
    "\n",
    "### MinMaxScaler\n",
    "Deja los valores en el intervalo $[0,1]$.\n",
    "### RobustScaler\n",
    "\n",
    "$$\\frac{x_i - Q_1(x)}{Q_3(x) - Q_1(x)}$$\n",
    "\n",
    "Duda: ¿valores negativos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da8ba74-9139-466c-a5c5-25607953ba76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Suponemos que 'df_int' es el DataFrame original con los datos\n",
    "\n",
    "# Seleccionar las columnas numéricas\n",
    "numerical_cols = df_int.select_dtypes(include=('float', 'int')).columns\n",
    "\n",
    "# Escalar los datos con StandardScaler (puedes cambiarlo a MinMaxScaler o RobustScaler si prefieres)\n",
    "scaler = RobustScaler()\n",
    "df_scaled = scaler.fit_transform(df_int[numerical_cols])\n",
    "\n",
    "# Convertir la salida de la escala (numpy ndarray) de nuevo a un DataFrame\n",
    "df_scaled = pd.DataFrame(df_scaled, columns=numerical_cols)\n",
    "\n",
    "# Combinar las columnas escaladas con las columnas no numéricas (no escaladas)\n",
    "df_int = pd.concat([df_int.drop(columns=numerical_cols).reset_index(drop=True), \n",
    "                           df_scaled.reset_index(drop=True)], axis=1)\n",
    "\n",
    "df_int[numerical_cols] = df_int[numerical_cols].clip(lower=0)\n",
    "df_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d22e8d-a2ae-4410-9973-fa421c04d172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir la fecha a timestamp con la hora y zona horaria (en segundos desde la época de Unix)\n",
    "# df_int['date'] = (df_int['date'] - pd.Timestamp('1970-01-01 00:00:00+00:00')) // pd.Timedelta('1s')\n",
    "# df_int['date'] = df_int['date'].astype(np.float64)\n",
    "df_int_sin_date = df_int.drop(columns=['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c1a45a-c7da-4101-ae59-fa66fc7bbc4e",
   "metadata": {},
   "source": [
    "## Aumentar tamaño de la muestra +200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed62f007-963b-4c76-815a-25f6cac7f14c",
   "metadata": {},
   "source": [
    "### Con GAN (Generative Adversarial Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1fedaa-4ad6-47ff-8725-a6d5d2b41206",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.init as init\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 256)\n",
    "        self.fc3 = nn.Linear(256, 512)\n",
    "        self.fc4 = nn.Linear(512, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        # Inicialización de los pesos\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        # Inicialización de los pesos de las capas usando una distribución normal\n",
    "        for layer in self.children():\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                init.normal_(layer.weight, mean=0.0, std=0.02)  # Inicialización normal\n",
    "                if layer.bias is not None:\n",
    "                    init.constant_(layer.bias, 0)\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.relu(self.fc1(z))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.tanh(self.fc4(x))  # Salida entre -1 y 1\n",
    "        return x\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.fc4 = nn.Linear(128, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.sigmoid(self.fc4(x))  # Salida entre 0 y 1\n",
    "        return x\n",
    "\n",
    "\n",
    "# Dimensiones\n",
    "latent_dim = 200 \n",
    "input_dim = df_int_sin_date.shape[1]  # Número de características en los datos\n",
    "\n",
    "# Crear el generador y el discriminador\n",
    "generator = Generator(latent_dim, input_dim)\n",
    "discriminator = Discriminator(input_dim)\n",
    "\n",
    "# Definir las funciones de pérdida y optimizadores\n",
    "adversarial_loss = nn.BCELoss()  # Pérdida binaria para GAN\n",
    "optimizer_g = torch.optim.Adam(generator.parameters(), lr=0.0001, betas=(0.5, 0.999))  # Tasa de aprendizaje más baja\n",
    "optimizer_d = torch.optim.Adam(discriminator.parameters(), lr=0.0001, betas=(0.5, 0.999))  # Tasa de aprendizaje más baja\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 20\n",
    "\n",
    "# Convertir los datos escalados a un tensor de PyTorch\n",
    "df_tensor = torch.tensor(df_int_sin_date.values, dtype=torch.float32)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range(0, len(df_tensor), batch_size):\n",
    "        real_data = df_tensor[i:i+batch_size]\n",
    "        batch_size = real_data.size(0)\n",
    "\n",
    "        # Etiquetas reales y falsas\n",
    "        real_labels = torch.ones(batch_size, 1)\n",
    "        fake_labels = torch.zeros(batch_size, 1)\n",
    "\n",
    "        # ===== Entrenar el discriminador =====\n",
    "        optimizer_d.zero_grad()\n",
    "\n",
    "        # Pérdida para datos reales\n",
    "        real_preds = discriminator(real_data)\n",
    "        real_loss = adversarial_loss(real_preds, real_labels)\n",
    "\n",
    "        # Pérdida para datos generados\n",
    "        z = torch.randn(batch_size, latent_dim)\n",
    "        fake_data = generator(z)\n",
    "        fake_preds = discriminator(fake_data.detach())  # No calcular gradientes para los datos generados\n",
    "        fake_loss = adversarial_loss(fake_preds, fake_labels)\n",
    "\n",
    "        # Pérdida total para el discriminador\n",
    "        d_loss = real_loss + fake_loss\n",
    "        d_loss.backward()\n",
    "        optimizer_d.step()\n",
    "\n",
    "        # ===== Entrenar el generador =====\n",
    "        optimizer_g.zero_grad()\n",
    "\n",
    "        # Intentar que el generador engañe al discriminador\n",
    "        fake_preds = discriminator(fake_data)\n",
    "        g_loss = adversarial_loss(fake_preds, real_labels)  # Queremos que el discriminador piense que los datos generados son reales\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizer_g.step()\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch [{epoch}/{epochs}] | D Loss: {d_loss.item()} | G Loss: {g_loss.item()}\")\n",
    "\n",
    "# Generar nuevos datos\n",
    "z = torch.randn(200, latent_dim)  # 200 nuevas filas\n",
    "generated_data = generator(z).detach().numpy()\n",
    "\n",
    "# Convertir a DataFrame\n",
    "generated_df = pd.DataFrame(generated_data, columns=df_int_sin_date.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5a1520-28ba-4af3-ae82-df7e2df9fa00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Mostrar los nuevos datos generados\n",
    "generated_df = generated_df.clip(lower=0)\n",
    "generated_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236ed409-b54b-47c1-857d-d14d1d56a7cd",
   "metadata": {},
   "source": [
    "### Con Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428de943-b6bf-44f4-9568-07d1af8055a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Dataset personalizado\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, data, feature_cols):\n",
    "        self.features = torch.tensor(data[feature_cols].values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx]\n",
    "\n",
    "# Modelo Transformer básico\n",
    "class TabularTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, model_dim=64, num_heads=2, num_layers=2):\n",
    "        super(TabularTransformer, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, model_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=model_dim, nhead=num_heads, dim_feedforward=512, dropout=0.1\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.decoder = nn.Linear(model_dim, input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x).unsqueeze(1)  # Añade una dimensión temporal\n",
    "        x = self.transformer(x).squeeze(1)  # Reduce la dimensión temporal\n",
    "        return self.decoder(x)\n",
    "\n",
    "# Preparar datos\n",
    "feature_cols = list(df_int_sin_date.columns)  # Todas las columnas como características\n",
    "train_dataset = TabularDataset(df_int_sin_date, feature_cols)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Modelo\n",
    "input_dim = len(feature_cols)\n",
    "model = TabularTransformer(input_dim=input_dim)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Entrenar el modelo\n",
    "epochs = 10\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "        loss = criterion(output, batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "# Generar nuevas filas\n",
    "model.eval()\n",
    "n_generate = 200\n",
    "with torch.no_grad():\n",
    "    noise = torch.randn(n_generate, input_dim)  # Ruido aleatorio como entrada\n",
    "    generated_features = model(noise).numpy()\n",
    "\n",
    "# Crear DataFrame con las nuevas filas generadas\n",
    "generated_data = pd.DataFrame(generated_features, columns=feature_cols)\n",
    "generated_data = generated_data.clip(lower=0)\n",
    "\n",
    "\n",
    "print(f\"Tamaño del DataFrame original: {df_int_sin_date.shape}\")\n",
    "print(f\"Tamaño del DataFrame combinado: {df_int_sin_date_aug.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed01a9dc-73c4-43aa-8515-2dc6d566cdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_data = generated_data.clip(lower=0)\n",
    "generated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c699ca1-3f05-4387-aa3e-d215542ff902",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "start_date = pd.to_datetime(\"2023-06-27 01:00:00+00:00\")\n",
    "\n",
    "new_dates = pd.date_range(start=start_date, periods=200, freq='H')\n",
    "\n",
    "generated_data['date'] = new_dates\n",
    "\n",
    "generated_data.insert(0, 'date', generated_data.pop('date'))\n",
    "\n",
    "df_int_aug = pd.concat([df_int, generated_data], ignore_index=True)\n",
    "df_int_aug"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482f24c5-611a-46c2-8938-839141677952",
   "metadata": {},
   "source": [
    "## Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1dcbd0-a174-4410-98f9-8500ef1da7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(df_int[\"date\"]).issubset(set(df_orig[\"date\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cc6996-dfe8-4fce-af5c-d0572e4a4bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig[df_orig[\"date\"] == df_int_aug[\"date\"][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa0bf5d-8e2a-42fd-b2eb-aaffbbf43257",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig[df_orig[\"date\"] == df_int_aug[\"date\"][980]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23abf276-bd9b-43ce-aded-abe58e09022c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig[df_orig[\"date\"] == df_int_aug[\"date\"][1534]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c3bd83-2739-4c79-83ab-277a07b793db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df_orig, df_int_aug, how=\"left\", on=[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8682fa23-59af-4bc9-9bfa-381605cee60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuevas_columnas = df_int_aug.columns.difference(df_orig.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5cf48e-1569-4b2f-933b-af5485bbb1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuevas_columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6645700e-dee9-4c53-adee-bd2327fcb72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[5798:5850, [\"date\"] + list(nuevas_columnas) + columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a64cf0f-6301-4ad1-a26b-9a4dfd321203",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df[\"eBC_tot\"].notna())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5a9ca7-9fde-450e-85d1-5db2de05c03e",
   "metadata": {},
   "source": [
    "## Generación de datos sintéticos para black carbon ebc_tot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c439de8-4351-43c9-9b75-4a429b03fa53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Identificar las filas con valores faltantes en 'eBC_tot'\n",
    "df_complete = df[df['eBC_tot'].notna()]  # Filas donde eBC_tot tiene datos\n",
    "df_missing = df[df['eBC_tot'].isna()]    # Filas donde eBC_tot es NaN\n",
    "# Las columnas a usar para predecir eBC_tot\n",
    "feature_cols = [col for col in df.columns if col not in ('eBC_tot', 'date')]\n",
    "\n",
    "# Dataset para entrenamiento (solo con datos completos)\n",
    "train_data = df_complete[feature_cols]\n",
    "train_labels = df_complete['eBC_tot']\n",
    "\n",
    "# Convertir a tensores para PyTorch\n",
    "train_data_tensor = torch.tensor(train_data.values, dtype=torch.float32)\n",
    "train_labels_tensor = torch.tensor(train_labels.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# Crear Dataset personalizado para entrenamiento\n",
    "class TabularDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "train_dataset = TabularDataset(train_data_tensor, train_labels_tensor)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c54ba46-97fb-4b70-a849-6b10b9081a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Modelo Transformer para imputación\n",
    "class TabularTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, model_dim=64, num_heads=2, num_layers=2):\n",
    "        super(TabularTransformer, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, model_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=model_dim, nhead=num_heads, dim_feedforward=512, dropout=0.1\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.decoder = nn.Linear(model_dim, 1)  # Para predecir un valor (eBC_tot)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x).unsqueeze(1)  # Añadir dimensión temporal\n",
    "        x = self.transformer(x).squeeze(1)  # Reducir la dimensión temporal\n",
    "        return self.decoder(x)  # Predicción de un solo valor (eBC_tot)\n",
    "\n",
    "# Dimensiones del modelo\n",
    "input_dim = len(feature_cols)\n",
    "model = TabularTransformer(input_dim=input_dim)\n",
    "\n",
    "# Pérdida y optimizador\n",
    "criterion = nn.MSELoss()  # Usamos MSE para regresión\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Entrenar el modelo\n",
    "epochs = 10\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for batch_data, batch_labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch_data)\n",
    "        loss = criterion(output, batch_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss/len(train_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5086d52-7661-46d3-9af4-976078a22095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predecir los valores faltantes de eBC_tot usando el modelo\n",
    "df_missing_data = df_missing[feature_cols]\n",
    "df_missing_data_tensor = torch.tensor(df_missing_data.values, dtype=torch.float32)\n",
    "\n",
    "model.eval()  # Modo de evaluación\n",
    "with torch.no_grad():\n",
    "    predicted_values = model(df_missing_data_tensor).squeeze(1).numpy()\n",
    "\n",
    "# Imputar los valores faltantes con las predicciones\n",
    "df_missing['eBC_tot'] = predicted_values\n",
    "\n",
    "# Concatenar los datos imputados con los datos completos\n",
    "df_imputed = pd.concat([df_complete, df_missing], axis=0)\n",
    "df_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c87b83-aaad-41f2-bc28-7979982b17b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df_imputed[\"eBC_tot\"].notna())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371cea2f-8d38-4682-ba70-643a630624ed",
   "metadata": {},
   "source": [
    "## Matriz de correlación entre vehiculos y variables de contaminación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0717fd31-293d-4eb4-9603-544ca75b3efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular la matriz de correlación\n",
    "cols = [col for col in df_imputed.columns if col != 'date']\n",
    "correlation_matrix = df_imputed[cols].corr()\n",
    "\n",
    "# Mostrar la matriz\n",
    "plt.figure(figsize=(40, 32))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Matriz de Correlación entre Variables de Contaminación y Meteorológicas')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
